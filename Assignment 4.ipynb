{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 4: Pipelines and Hyperparameter Tuning (32 total marks)\n",
    "### Due: November 22 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models and evaluate the results. More details for each step can be found below.\n",
    "\n",
    "### You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "## Step 1: Data Input (4 marks)\n",
    "\n",
    "Import the dataset you will be using. You can download the dataset onto your computer and read it in using pandas, or download it directly from the website. Answer the questions below about the dataset you selected. \n",
    "\n",
    "To find a dataset, you can use the resources listed in the notes. The dataset can be numerical, categorical, text-based or mixed. If you want help finding a particular dataset related to your interests, please email the instructor.\n",
    "\n",
    "**You cannot use a dataset that was used for a previous assignment or in class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 105, 'name': 'Congressional Voting Records', 'repository_url': 'https://archive.ics.uci.edu/dataset/105/congressional+voting+records', 'data_url': 'https://archive.ics.uci.edu/static/public/105/data.csv', 'abstract': '1984 United Stated Congressional Voting Records; Classify as Republican or Democrat', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 435, 'num_features': 16, 'feature_types': ['Categorical'], 'demographics': [], 'target_col': ['Class'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1987, 'last_updated': 'Mon Apr 27 1987', 'dataset_doi': '10.24432/C5C01P', 'creators': [], 'intro_paper': None, 'additional_info': {'summary': 'This data set includes votes for each of the U.S. House of Representatives Congressmen on the 16 key votes identified by the CQA.  The CQA lists nine different types of votes: voted for, paired for, and announced for (these three simplified to yea), voted against, paired against, and announced against (these three simplified to nay), voted present, voted present to avoid conflict of interest, and did not vote or otherwise make a position known (these three simplified to an unknown disposition).', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '   1. Class Name: 2 (democrat, republican)\\r\\n   2. handicapped-infants: 2 (y,n)\\r\\n   3. water-project-cost-sharing: 2 (y,n)\\r\\n   4. adoption-of-the-budget-resolution: 2 (y,n)\\r\\n   5. physician-fee-freeze: 2 (y,n)\\r\\n   6. el-salvador-aid: 2 (y,n)\\r\\n   7. religious-groups-in-schools: 2 (y,n)\\r\\n   8. anti-satellite-test-ban: 2 (y,n)\\r\\n   9. aid-to-nicaraguan-contras: 2 (y,n)\\r\\n  10. mx-missile: 2 (y,n)\\r\\n  11. immigration: 2 (y,n)\\r\\n  12. synfuels-corporation-cutback: 2 (y,n)\\r\\n  13. education-spending: 2 (y,n)\\r\\n  14. superfund-right-to-sue: 2 (y,n)\\r\\n  15. crime: 2 (y,n)\\r\\n  16. duty-free-exports: 2 (y,n)\\r\\n  17. export-administration-act-south-africa: 2 (y,n)', 'citation': None}}\n",
      "                                      name     role         type demographic  \\\n",
      "0                                    Class   Target  Categorical        None   \n",
      "1                      handicapped-infants  Feature  Categorical        None   \n",
      "2               water-project-cost-sharing  Feature  Categorical        None   \n",
      "3        adoption-of-the-budget-resolution  Feature  Categorical        None   \n",
      "4                     physician-fee-freeze  Feature  Categorical        None   \n",
      "5                          el-salvador-aid  Feature  Categorical        None   \n",
      "6              religious-groups-in-schools  Feature  Categorical        None   \n",
      "7                  anti-satellite-test-ban  Feature  Categorical        None   \n",
      "8                aid-to-nicaraguan-contras  Feature  Categorical        None   \n",
      "9                               mx-missile  Feature  Categorical        None   \n",
      "10                             immigration  Feature  Categorical        None   \n",
      "11            synfuels-corporation-cutback  Feature  Categorical        None   \n",
      "12                      education-spending  Feature  Categorical        None   \n",
      "13                  superfund-right-to-sue  Feature  Categorical        None   \n",
      "14                                   crime  Feature  Categorical        None   \n",
      "15                       duty-free-exports  Feature  Categorical        None   \n",
      "16  export-administration-act-south-africa  Feature  Categorical        None   \n",
      "\n",
      "   description units missing_values  \n",
      "0         None  None             no  \n",
      "1         None  None            yes  \n",
      "2         None  None            yes  \n",
      "3         None  None            yes  \n",
      "4         None  None            yes  \n",
      "5         None  None            yes  \n",
      "6         None  None            yes  \n",
      "7         None  None            yes  \n",
      "8         None  None            yes  \n",
      "9         None  None            yes  \n",
      "10        None  None            yes  \n",
      "11        None  None            yes  \n",
      "12        None  None            yes  \n",
      "13        None  None            yes  \n",
      "14        None  None            yes  \n",
      "15        None  None            yes  \n",
      "16        None  None            yes  \n"
     ]
    }
   ],
   "source": [
    "# Import dataset (1 mark)\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "congressional_voting_records = fetch_ucirepo(id=105) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = congressional_voting_records.data.features \n",
    "y = congressional_voting_records.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(congressional_voting_records.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(congressional_voting_records.variables) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20316765",
   "metadata": {},
   "source": [
    "### Questions (3 marks)\n",
    "\n",
    "1. (1 mark) What is the source of your dataset?\n",
    "\n",
    "The dataset comes from the UCI Machine Learning Repository, a popular source for machine learning datasets. https://archive.ics.uci.edu/dataset/105/congressional+voting+records\n",
    "\n",
    "2. (1 mark) Why did you pick this particular dataset?\n",
    "\n",
    "This dataset have been chosen because it provides a clear example of a binary classification problem with real-world political data. It's also relatively simple in structure, making it suitable for learning and demonstrating basic concepts in data preprocessing and machine learning.\n",
    "\n",
    "3. (1 mark) Was there anything challenging about finding a dataset that you wanted to use?\n",
    "\n",
    "Challenges include ensuring the dataset is relevant, not overly complex for the learning objectives, and has well-documented features and instances. It's also important that the dataset is free of ethical concerns and privacy issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "## Step 2: Data Processing (5 marks)\n",
    "\n",
    "The next step is to process your data. Implement the following steps as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc244d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column after handling:\n",
      " handicapped-infants                       0\n",
      "water-project-cost-sharing                0\n",
      "adoption-of-the-budget-resolution         0\n",
      "physician-fee-freeze                      0\n",
      "el-salvador-aid                           0\n",
      "religious-groups-in-schools               0\n",
      "anti-satellite-test-ban                   0\n",
      "aid-to-nicaraguan-contras                 0\n",
      "mx-missile                                0\n",
      "immigration                               0\n",
      "synfuels-corporation-cutback              0\n",
      "education-spending                        0\n",
      "superfund-right-to-sue                    0\n",
      "crime                                     0\n",
      "duty-free-exports                         0\n",
      "export-administration-act-south-africa    0\n",
      "dtype: int64\n",
      "Data types of each column after encoding:\n",
      " handicapped-infants                       int64\n",
      "water-project-cost-sharing                int64\n",
      "adoption-of-the-budget-resolution         int64\n",
      "physician-fee-freeze                      int64\n",
      "el-salvador-aid                           int64\n",
      "religious-groups-in-schools               int64\n",
      "anti-satellite-test-ban                   int64\n",
      "aid-to-nicaraguan-contras                 int64\n",
      "mx-missile                                int64\n",
      "immigration                               int64\n",
      "synfuels-corporation-cutback              int64\n",
      "education-spending                        int64\n",
      "superfund-right-to-sue                    int64\n",
      "crime                                     int64\n",
      "duty-free-exports                         int64\n",
      "export-administration-act-south-africa    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Clean data (if needed)\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Fetch the dataset\n",
    "congressional_voting_records = fetch_ucirepo(id=105)\n",
    "\n",
    "# Data (assuming it's a DataFrame)\n",
    "df = congressional_voting_records.data.features\n",
    "\n",
    "# Creating a copy of the DataFrame for cleaning and encoding\n",
    "df_cleaned = df.drop_duplicates().copy()\n",
    "\n",
    "# Handling missing values (example: fill with the most frequent value in each column)\n",
    "for column in df_cleaned.columns:\n",
    "    most_frequent = df_cleaned[column].mode()[0]\n",
    "    df_cleaned[column] = df_cleaned[column].fillna(most_frequent)\n",
    "\n",
    "# Checking for missing values in each column after handling them\n",
    "missing_values_after = df_cleaned.isnull().sum()\n",
    "print(\"Missing values in each column after handling:\\n\", missing_values_after)\n",
    "\n",
    "# Encode categorical data (example: Convert 'yes'/'no' votes to 1/0)\n",
    "yes_no_columns = df_cleaned.columns  # List all columns that need encoding\n",
    "for column in yes_no_columns:\n",
    "    df_cleaned[column] = df_cleaned[column].map({'y': 1, 'n': 0})\n",
    "\n",
    "# Checking the data types of each column after encoding\n",
    "print(\"Data types of each column after encoding:\\n\", df_cleaned.dtypes)\n",
    "\n",
    "# Further analysis and processing steps...\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a8c127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "Training set shape: (348, 16)\n",
      "Testing set shape: (87, 16)\n"
     ]
    }
   ],
   "source": [
    "# Implement preprocessing steps. Remember to use ColumnTransformer if more than one preprocessing method is needed\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fetch the dataset\n",
    "congressional_voting_records = fetch_ucirepo(id=105)\n",
    "\n",
    "# Data (as pandas dataframes)\n",
    "X = congressional_voting_records.data.features\n",
    "y = congressional_voting_records.data.targets\n",
    "\n",
    "# Convert the target DataFrame to a NumPy array and then flatten it to a 1D array\n",
    "y_array = y.values.ravel()\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_array)\n",
    "\n",
    "# Replace 'y' and 'n' with 1 and 0, and use np.nan for missing values in features\n",
    "X_encoded = X.replace({'y': 1, 'n': 0, '?': np.nan})\n",
    "\n",
    "# Apply SimpleImputer to handle the missing values in features\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X_encoded), columns=X_encoded.columns)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print confirmation and some details about the preprocessed data\n",
    "print(\"Preprocessing completed.\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c46b7",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "\n",
    "1. (1 mark) Were there any missing/null values in your dataset? If yes, how did you replace them and why? If no, describe how you would've replaced them and why.\n",
    "\n",
    "The dataset contains missing/null values represented as '?'. These were replaced with the most frequent value in each column, a common practice known as mode imputation. This method maintains the overall distribution of the data and is suitable for categorical data.\n",
    "\n",
    "2. (1 mark) What type of data do you have? What preprocessing methods would you have to apply based on your data types?\n",
    "\n",
    "The data is categorical (votes recorded as 'yes', 'no', or missing). Preprocessing included encoding 'yes' and 'no' as binary values (1 and 0) and handling missing values. If numerical data were present, methods like normalization or standardization might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "## Step 3: Implement Machine Learning Model (11 marks)\n",
    "\n",
    "In this section, you will implement three different supervised learning models (one linear and two non-linear) of your choice. You will use a pipeline to help you decide which model and hyperparameters work best. It is up to you to select what models to use and what hyperparameters to test. You can use the class examples for guidance. You must print out the best model parameters and results after the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5558a776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best parameters for rf:  {'classifier__max_depth': None, 'classifier__n_estimators': 200}\n",
      "Best score for rf:  0.9539958592132505\n",
      "Accuracy for rf:  0.9770114942528736\n",
      "Confusion Matrix for rf:\n",
      " [[55  1]\n",
      " [ 1 30]]\n",
      "Classification Report for rf:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        56\n",
      "           1       0.97      0.97      0.97        31\n",
      "\n",
      "    accuracy                           0.98        87\n",
      "   macro avg       0.97      0.97      0.97        87\n",
      "weighted avg       0.98      0.98      0.98        87\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best parameters for lr:  {'classifier__C': 1}\n",
      "Best score for lr:  0.9655072463768116\n",
      "Accuracy for lr:  0.9540229885057471\n",
      "Confusion Matrix for lr:\n",
      " [[54  2]\n",
      " [ 2 29]]\n",
      "Classification Report for lr:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        56\n",
      "           1       0.94      0.94      0.94        31\n",
      "\n",
      "    accuracy                           0.95        87\n",
      "   macro avg       0.95      0.95      0.95        87\n",
      "weighted avg       0.95      0.95      0.95        87\n",
      "\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best parameters for svc:  {'classifier__C': 10, 'classifier__kernel': 'rbf'}\n",
      "Best score for svc:  0.9626086956521739\n",
      "Accuracy for svc:  0.9310344827586207\n",
      "Confusion Matrix for svc:\n",
      " [[54  2]\n",
      " [ 4 27]]\n",
      "Classification Report for svc:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.95        56\n",
      "           1       0.93      0.87      0.90        31\n",
      "\n",
      "    accuracy                           0.93        87\n",
      "   macro avg       0.93      0.92      0.92        87\n",
      "weighted avg       0.93      0.93      0.93        87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implement pipeline and grid search here. Can add more code blocks if necessary\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Define pipelines for each model\n",
    "pipelines = {\n",
    "    'rf': Pipeline([('classifier', RandomForestClassifier(random_state=42))]),\n",
    "    'lr': Pipeline([('classifier', LogisticRegression())]),\n",
    "    'svc': Pipeline([('classifier', SVC())])\n",
    "}\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'rf': {'classifier__n_estimators': [100, 200], 'classifier__max_depth': [None, 10, 20]},\n",
    "    'lr': {'classifier__C': [0.1, 1, 10]},\n",
    "    'svc': {'classifier__C': [0.1, 1, 10], 'classifier__kernel': ['linear', 'rbf']}\n",
    "}\n",
    "\n",
    "# Implement GridSearchCV for each model and compare results\n",
    "best_models = {}\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    grid_search = GridSearchCV(pipeline, param_grids[model_name], cv=5, verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = grid_search\n",
    "\n",
    "    # Print best parameters and score\n",
    "    print(f\"Best parameters for {model_name}: \", grid_search.best_params_)\n",
    "    print(f\"Best score for {model_name}: \", grid_search.best_score_)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(f\"Accuracy for {model_name}: \", accuracy)\n",
    "    print(f\"Confusion Matrix for {model_name}:\\n\", conf_matrix)\n",
    "    print(f\"Classification Report for {model_name}:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd7075",
   "metadata": {},
   "source": [
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Do you need regression or classification models for your dataset?\n",
    "\n",
    "For the Congressional Voting Records dataset, classification models are needed. The target variable in this dataset is categorical, representing political party affiliations (Democrat or Republican). Classification models are designed to predict categorical outcomes, making them suitable for this kind of data.\n",
    "\n",
    "2. (2 marks) Which models did you select for testing and why?\n",
    "\n",
    "The models selected for testing were RandomForestClassifier, Logistic Regression, and Support Vector Classifier (SVC).\n",
    "\n",
    "*RandomForestClassifier: This was chosen due to its ability to handle categorical data effectively and its robustness against overfitting. RandomForest, being an ensemble method, generally performs well on a variety of datasets.\n",
    "\n",
    "*Logistic Regression: As a linear model, Logistic Regression is straightforward, interpretable, and particularly effective for binary classification problems. It was chosen to compare how a simpler linear approach performs against more complex models.\n",
    "\n",
    "*SVC: Support Vector Classifier was selected to represent another type of non-linear model. With different kernel options (linear and RBF), it provides a way to test both linear and non-linear decision boundaries.\n",
    "\n",
    "1. (2 marks) Which model worked the best? Does this make sense based on the theory discussed in the course and the context of your dataset?\n",
    "\n",
    "*To determine which model worked best, we would typically compare the grid search scores and the testing accuracy of the models. The best model is the one that achieves the highest accuracy on the test dataset.\n",
    "\n",
    "*RandomForestClassifier might have an edge due to its ability to capture complex patterns in the data without requiring feature scaling or extensive data preprocessing. Its performance generally aligns with the expectation for datasets with categorical features. \n",
    "\n",
    "*Logistic Regression, being simpler, might perform well if the decision boundary is approximately linear.\n",
    "\n",
    "*SVC's performance can vary significantly based on the choice of kernel and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "## Step 4: Validate Model (6 marks)\n",
    "\n",
    "Use the testing set to calculate the testing accuracy for the best model determined in Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69e64c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model based on grid search: lr\n",
      "Testing Accuracy: 0.9540229885057471\n"
     ]
    }
   ],
   "source": [
    "# Calculate testing accuracy (1 mark)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Identify the best model from grid search\n",
    "best_model_name = max(best_models, key=lambda model: best_models[model].best_score_)\n",
    "best_model = best_models[best_model_name].best_estimator_\n",
    "\n",
    "print(f\"Best model based on grid search: {best_model_name}\")\n",
    "\n",
    "# Predict on the test data using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4529ba",
   "metadata": {},
   "source": [
    "\n",
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Which accuracy metric did you choose? \n",
    "\n",
    "The chosen accuracy metric was the standard accuracy score, which is the proportion of correct predictions out of all predictions. This metric is straightforward and widely used, especially for binary classification tasks.\n",
    "\n",
    "2. (1 mark) How do these results compare to those in part 3? Did this model generalize well?\n",
    "\n",
    "*The results in Part 3 involved running GridSearchCV for three different models (RandomForestClassifier, Logistic Regression, and SVC) and evaluating their performance on the testing set.\n",
    "\n",
    "*The model that performed best in terms of grid search score (cross-validation score) and testing accuracy would be considered the best model. If this model also shows similar performance on both training (cross-validation) and testing sets, it suggests that the model has generalized well.\n",
    "\n",
    "3. (3 marks) Based on your results and the context of your dataset, did the best model perform \"well enough\" to be used out in the real-world? Why or why not? Do you have any suggestions for how you could improve this analysis?\n",
    "\n",
    "Real-World Applicability of the Best Model:\n",
    "\n",
    "*The real-world applicability of the best-performing model depends on its testing accuracy and the context of the dataset. If the best model shows high accuracy and the errors it makes are not critical in the application context (e.g., political analysis, prediction of voting behavior), it might be considered suitable for real-world use.\n",
    "\n",
    "*To further improve the analysis, you could experiment with more diverse hyperparameter tuning, try different preprocessing techniques, or explore ensemble methods that combine the strengths of various models.\n",
    "\n",
    "Reflecting on Part 3:\n",
    "\n",
    "*This part of the assignment likely provided valuable insights into the practical aspects of model selection and hyperparameter tuning. The use of GridSearchCV with multiple models would have offered an understanding of how different models and their parameters impact performance.\n",
    "\n",
    "*A potential challenge could have been managing the computational cost and time involved in extensive grid searches, especially with models like SVC that have multiple hyperparameters. The iterative process of tuning and evaluating models also requires careful consideration and interpretation of results.\n",
    "The experience of comparing different models and seeing how theoretical principles translate into practical outcomes is both enlightening and essential for developing a deeper understanding of machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "## Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "\n",
    "1. Where did you source your code?\n",
    "\n",
    "The dataset comes from the UCI Machine Learning Repository, a popular source for machine learning datasets. \n",
    "https://archive.ics.uci.edu/dataset/105/congressional+voting+records\n",
    "\n",
    "2. In what order did you complete the steps?\n",
    "\n",
    "*Data Loading: The first step involved fetching and loading the dataset using the ucimlrepo package.\n",
    "\n",
    "*Data Preprocessing: This included handling missing values, encoding categorical variables, and preparing the dataset for modeling.\n",
    "\n",
    "*Model Selection and Setup: Selection of models (RandomForest, Logistic Regression, and SVC) for testing, along with setting up pipelines for each.\n",
    "\n",
    "*Hyperparameter Tuning: Implementing GridSearchCV for each model to find the best parameters.\n",
    "Model Evaluation: Assessing model performance using accuracy and other metrics like confusion matrix and classification report on the testing set.\n",
    "\n",
    "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "\n",
    "* Yes, ChatGpt was used to understand the requirements and implementation better. \n",
    "* The prompts were the instructions and queries regarding the implementation of the machine learning models depending on the requirements.\n",
    "* The Modifications were done for specific details like the target variable and feature selection were based on assumptions and might need adjustment according to the exact dataset and task.\n",
    "\n",
    "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "\n",
    "* Lack of Specific Details: The primary challenge was the lack of specific details about the target variable and features in your dataset. Assumptions were made for these, which might not align perfectly with your actual use case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challenging, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "* Liked the implementation of the multiple Machine Learning Models.\n",
    "\n",
    "* While working on this assignment, I found it interesting and motivating to help with a machine learning task involving code generation. \n",
    "\n",
    "* Challenges include understanding the best practices for data preprocessing, selecting appropriate models, and tuning parameters for optimal performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
